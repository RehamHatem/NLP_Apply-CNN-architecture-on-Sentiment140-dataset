{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove Pre and Post Spaces\n",
        "    text = str(text).strip()\n",
        "    # Lower case\n",
        "    text = str(text).lower()\n",
        "    # New Line Characters with spaces\n",
        "    text = re.sub(r\"\\n\", r\" \", text)\n",
        "    # Tokenize the sentence\n",
        "    word_tokens = word_tokenize(text)\n",
        "    # Remove the punctuation and special characters from each individual word\n",
        "    cleaned_text = []\n",
        "    for word in word_tokens:\n",
        "        cleaned_text.append(\"\".join([char for char in word if char.isalnum()]))\n",
        "\n",
        "    # Specify the stop words list\n",
        "    stop_words = stopwords.words('english')\n",
        "    # Remove the stopwords and words containing less than 2 characters\n",
        "    text_tokens = [word for word in cleaned_text if (len(word) > 2) and (word not in stop_words)]\n",
        "    # Lemmatize\n",
        "    text = [lemmatizer.lemmatize(word) for word in text_tokens]\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to read CSV file and return the specified columns\n",
        "def read_csv_file(filename, columns, headers, encoding='utf-8'):\n",
        "    data = []\n",
        "    data.append(headers)\n",
        "    with open(filename, 'r', encoding=encoding) as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip the header row\n",
        "        for row in csv_reader:\n",
        "            # Preprocess text column\n",
        "            preprocessed_text = preprocess_text(row[5])\n",
        "            selected_row = [row[i] for i in columns]\n",
        "            selected_row[1] = preprocessed_text  # Replace original text with preprocessed text\n",
        "            data.append(selected_row)\n",
        "    return data\n",
        "\n",
        "# Define the column indices to extract\n",
        "columns_to_extract = [0, 5]\n",
        "custom_headers = [\"target\", \"text\"]\n",
        "\n",
        "# Reading the CSV file\n",
        "data = read_csv_file('/content/training.1600000.processed.noemoticon.csv', columns_to_extract, custom_headers, encoding='latin-1')\n",
        "\n",
        "# Extracting features (text) and labels (target)\n",
        "features = [row[1] for row in data[1:]]  # Exclude headers\n",
        "labels = [int(row[0]) for row in data[1:]]  # Exclude headers and convert to integers\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(\"Training set size:\", len(x_train))\n",
        "print(\"Testing set size:\", len(x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmI0D3U2lMZK",
        "outputId": "f939b931-9fcd-49d1-e97b-cfe073258f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1279999\n",
            "Testing set size: 320000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(\"Training Data Samples:\")\n",
        "for i in range(5):\n",
        "    print(\"Sample:\", i+1)\n",
        "    print(\"Text:\", x_train[i])\n",
        "    print(\"Label:\", y_train[i])\n",
        "    print()\n",
        "\n",
        "print(\"Testing Data Samples:\")\n",
        "for i in range(5):\n",
        "    print(\"Sample:\", i+1)\n",
        "    print(\"Text:\", x_test[i])\n",
        "    print(\"Label:\", y_test[i])\n",
        "    print()\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the combined labels and transform both training and testing labels\n",
        "y_combined = y_train + y_test\n",
        "encoder.fit(y_combined)\n",
        "\n",
        "y_train_encoded = encoder.transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "# Convert integer labels to one-hot encoded format\n",
        "num_classes = len(encoder.classes_)\n",
        "y_train_onehot = to_categorical(y_train_encoded, num_classes)\n",
        "y_test_onehot = to_categorical(y_test_encoded, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EararKYd0tg1",
        "outputId": "f21e98a6-dc9d-490d-9ca8-a38292fe634b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Samples:\n",
            "Sample: 1\n",
            "Text: ['otavolimed', 'second', 'get', 'back', 'haha']\n",
            "Label: 4\n",
            "\n",
            "Sample: 2\n",
            "Text: ['race', 'life', 'awesome']\n",
            "Label: 4\n",
            "\n",
            "Sample: 3\n",
            "Text: ['good', 'clothes', 'otherwise', 'could', 'even', 'awkward', 'already', 'lol']\n",
            "Label: 4\n",
            "\n",
            "Sample: 4\n",
            "Text: ['late', 'work', 'week', 'even', 'take', 'metro', 'guess', 'bus', 'affected', 'crash']\n",
            "Label: 0\n",
            "\n",
            "Sample: 5\n",
            "Text: ['selestina118', 'incredible', 'people', 'need', 'know', 'worry']\n",
            "Label: 4\n",
            "\n",
            "Testing Data Samples:\n",
            "Sample: 1\n",
            "Text: ['nkluvr4eva', 'poor', 'little', 'dumpling', 'holmdel', 'vids', 'really', 'trying', 'hope', 'dont', 'try', 'hard', 'tonight']\n",
            "Label: 0\n",
            "\n",
            "Sample: 2\n",
            "Text: ['bed', 'got', 'wake', 'hella', 'early', 'tomorrow', 'morning']\n",
            "Label: 0\n",
            "\n",
            "Sample: 3\n",
            "Text: ['havent', 'able', 'listen', 'yet', 'speaker', 'busted']\n",
            "Label: 0\n",
            "\n",
            "Sample: 4\n",
            "Text: ['remembers', 'solving', 'relatively', 'big', 'equation', 'two', 'unknown', 'total', 'pain', 'butt']\n",
            "Label: 0\n",
            "\n",
            "Sample: 5\n",
            "Text: ['ate', 'much', 'feel', 'sick']\n",
            "Label: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "embedding_dim=100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad sequences\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=max_len)\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "By8TOjdqCtJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_pad, y_train_onehot, epochs=2, batch_size=132,validation_data=(x_test_pad, y_test_onehot))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test_pad, y_test_onehot)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "KElyn_NDJuB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Define the model\n",
        "filter_sizes = [3, 5, 7]\n",
        "\n",
        "for filter_size in filter_sizes:\n",
        "    model = Sequential([\n",
        "        Embedding(max_words,100, input_length=max_len),\n",
        "        Conv1D(128, filter_size, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train_pad, y_train_onehot, epochs=2, batch_size=132, validation_data=(x_test_pad, y_test_onehot))\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test_pad, y_test_onehot)\n",
        "    print(f\"Test Loss (Filter Size {filter_size}):\", loss)\n",
        "    print(f\"Test Accuracy (Filter Size {filter_size}):\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33qqRN1SZAYk",
        "outputId": "4c53597b-3fb2-4391-fb70-27bfb103203d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "9697/9697 [==============================] - 896s 92ms/step - loss: 0.4749 - accuracy: 0.7723 - val_loss: 0.4577 - val_accuracy: 0.7818\n",
            "Epoch 2/2\n",
            "9697/9697 [==============================] - 926s 95ms/step - loss: 0.4429 - accuracy: 0.7919 - val_loss: 0.4531 - val_accuracy: 0.7844\n",
            "10000/10000 [==============================] - 74s 7ms/step - loss: 0.4531 - accuracy: 0.7844\n",
            "Test Loss (Filter Size 3): 0.4531400799751282\n",
            "Test Accuracy (Filter Size 3): 0.784375011920929\n",
            "Epoch 1/2\n",
            "9697/9697 [==============================] - 1245s 128ms/step - loss: 0.4757 - accuracy: 0.7720 - val_loss: 0.4586 - val_accuracy: 0.7800\n",
            "Epoch 2/2\n",
            "1280/9697 [==>...........................] - ETA: 16:48 - loss: 0.4422 - accuracy: 0.7925"
          ]
        }
      ]
    }
  ]
}
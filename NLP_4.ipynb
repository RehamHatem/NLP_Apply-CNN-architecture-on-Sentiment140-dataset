{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RehamHatem/NLP_Apply-CNN-architecture-on-Sentiment140-dataset/blob/main/NLP_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmI0D3U2lMZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821c8988-e5b3-42e6-c905-48f45cc2b1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove Pre and Post Spaces\n",
        "    text = str(text).strip()\n",
        "\n",
        "    # Lower case the entire text\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Substitute New Line Characters with spaces\n",
        "    text = re.sub(r\"\\n\", r\" \", text)\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    word_tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove the punctuation and special characters from each individual word\n",
        "    cleaned_text = []\n",
        "    for word in word_tokens:\n",
        "        cleaned_text.append(\"\".join([char for char in word if char.isalnum()]))\n",
        "\n",
        "    # Specify the stop words list\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    # Remove the stopwords and words containing less than 2 characters\n",
        "    text_tokens = [word for word in cleaned_text if (len(word) > 2) and (word not in stop_words)]\n",
        "\n",
        "    # Lemmatize each word in the word list\n",
        "    text = [lemmatizer.lemmatize(word) for word in text_tokens]\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to read CSV file and return the specified columns\n",
        "def read_csv_file(filename, columns, headers, encoding='utf-8'):\n",
        "    data = []\n",
        "    data.append(headers)  # Append custom headers\n",
        "    with open(filename, 'r', encoding=encoding) as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        next(csv_reader)  # Skip the header row\n",
        "        for row in csv_reader:\n",
        "            # Preprocess text column\n",
        "            preprocessed_text = preprocess_text(row[5])  # Assuming the text is in the sixth column (0-based index)\n",
        "            selected_row = [row[i] for i in columns]\n",
        "            selected_row[1] = preprocessed_text  # Replace original text with preprocessed text\n",
        "            data.append(selected_row)\n",
        "    return data\n",
        "\n",
        "# Define the column indices to extract\n",
        "columns_to_extract = [0, 5]  # Assuming \"Target\" is the first column and \"text\" is the sixth column\n",
        "custom_headers = [\"target\", \"text\"]  # Custom headers without \"ids\"\n",
        "\n",
        "# Reading the CSV file\n",
        "data = read_csv_file('/content/training.1600000.processed.noemoticon.csv', columns_to_extract, custom_headers, encoding='latin-1')\n",
        "\n",
        "# Extracting features (text) and labels (target)\n",
        "features = [row[1] for row in data[1:]]  # Exclude headers\n",
        "labels = [int(row[0]) for row in data[1:]]  # Exclude headers and convert to integers\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the sizes of the training and testing sets\n",
        "print(\"Training set size:\", len(x_train))\n",
        "print(\"Testing set size:\", len(x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qg1-XZlEGBO",
        "outputId": "895a7ac7-c557-4fa0-a464-45f88e8405db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 1279999\n",
            "Testing set size: 320000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EararKYd0tg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6523512-df1d-4687-84a0-dca106a01575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Samples:\n",
            "Sample: 1\n",
            "Text: ['otavolimed', 'second', 'get', 'back', 'haha']\n",
            "Label: 4\n",
            "\n",
            "Sample: 2\n",
            "Text: ['race', 'life', 'awesome']\n",
            "Label: 4\n",
            "\n",
            "Sample: 3\n",
            "Text: ['good', 'clothes', 'otherwise', 'could', 'even', 'awkward', 'already', 'lol']\n",
            "Label: 4\n",
            "\n",
            "Sample: 4\n",
            "Text: ['late', 'work', 'week', 'even', 'take', 'metro', 'guess', 'bus', 'affected', 'crash']\n",
            "Label: 0\n",
            "\n",
            "Sample: 5\n",
            "Text: ['selestina118', 'incredible', 'people', 'need', 'know', 'worry']\n",
            "Label: 4\n",
            "\n",
            "Testing Data Samples:\n",
            "Sample: 1\n",
            "Text: ['nkluvr4eva', 'poor', 'little', 'dumpling', 'holmdel', 'vids', 'really', 'trying', 'hope', 'dont', 'try', 'hard', 'tonight']\n",
            "Label: 0\n",
            "\n",
            "Sample: 2\n",
            "Text: ['bed', 'got', 'wake', 'hella', 'early', 'tomorrow', 'morning']\n",
            "Label: 0\n",
            "\n",
            "Sample: 3\n",
            "Text: ['havent', 'able', 'listen', 'yet', 'speaker', 'busted']\n",
            "Label: 0\n",
            "\n",
            "Sample: 4\n",
            "Text: ['remembers', 'solving', 'relatively', 'big', 'equation', 'two', 'unknown', 'total', 'pain', 'butt']\n",
            "Label: 0\n",
            "\n",
            "Sample: 5\n",
            "Text: ['ate', 'much', 'feel', 'sick']\n",
            "Label: 0\n",
            "\n",
            "Shape of encoded training labels: (1279999, 3)\n",
            "Shape of encoded testing labels: (320000, 3)\n"
          ]
        }
      ],
      "source": [
        "# Printing samples from training data\n",
        "print(\"Training Data Samples:\")\n",
        "for i in range(5):\n",
        "    print(\"Sample:\", i+1)\n",
        "    print(\"Text:\", x_train[i])\n",
        "    print(\"Label:\", y_train[i])\n",
        "    print()\n",
        "\n",
        "# Printing samples from testing data\n",
        "print(\"Testing Data Samples:\")\n",
        "for i in range(5):\n",
        "    print(\"Sample:\", i+1)\n",
        "    print(\"Text:\", x_test[i])\n",
        "    print(\"Label:\", y_test[i])\n",
        "    print()\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder on the combined labels and transform both training and testing labels\n",
        "y_combined = y_train + y_test\n",
        "encoder.fit(y_combined)\n",
        "\n",
        "y_train_encoded = encoder.transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n",
        "\n",
        "# Convert integer labels to one-hot encoded format\n",
        "num_classes = 3\n",
        "y_train_onehot = to_categorical(y_train_encoded, num_classes)\n",
        "y_test_onehot = to_categorical(y_test_encoded, num_classes)\n",
        "\n",
        "# # Print the shape of encoded labels\n",
        "print(\"Shape of encoded training labels:\", y_train_onehot.shape)\n",
        "print(\"Shape of encoded testing labels:\", y_test_onehot.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By8TOjdqCtJg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Assuming x_train, x_test, y_train, y_test are already defined\n",
        "\n",
        "# Tokenize text data\n",
        "max_words = 100000  # Assuming you want to consider only the top 10,000 words\n",
        "max_len = 100  # Maximum sequence length\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad sequences\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=max_len)\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=max_len)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\":\", y_train_onehot[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxYURNz_pi8-",
        "outputId": "29cc1ce0-bc7b-418e-d43f-5644b0d2d835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": [[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KElyn_NDJuB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff059e2-aa5f-405c-e5ad-4719f9428110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "9697/9697 [==============================] - 107s 11ms/step - loss: 0.4704 - accuracy: 0.7757 - val_loss: 0.4497 - val_accuracy: 0.7883\n",
            "Epoch 2/2\n",
            "9697/9697 [==============================] - 91s 9ms/step - loss: 0.4050 - accuracy: 0.8159 - val_loss: 0.4542 - val_accuracy: 0.7875\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Embedding(max_words, 100, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')  # Softmax activation for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train_pad, y_train_onehot, epochs=2, batch_size=132,validation_data=(x_test_pad, y_test_onehot))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ey9ECaAaLV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacc4f89-11bd-4866-ef87-8923dd96f52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/10000 [==============================] - 30s 3ms/step - loss: 0.4536 - accuracy: 0.7853\n",
            "Test Loss: 0.45364126563072205\n",
            "Test Accuracy: 0.7852656245231628\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test_pad, y_test_onehot)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33qqRN1SZAYk",
        "outputId": "5983f6d4-1e78-4796-a534-75a38a00becd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "9697/9697 [==============================] - 101s 10ms/step - loss: 0.4705 - accuracy: 0.7755 - val_loss: 0.4462 - val_accuracy: 0.7897\n",
            "Epoch 2/3\n",
            "9697/9697 [==============================] - 79s 8ms/step - loss: 0.4040 - accuracy: 0.8165 - val_loss: 0.4529 - val_accuracy: 0.7887\n",
            "Epoch 3/3\n",
            "9697/9697 [==============================] - 81s 8ms/step - loss: 0.3399 - accuracy: 0.8504 - val_loss: 0.4765 - val_accuracy: 0.7802\n",
            "10000/10000 [==============================] - 28s 3ms/step - loss: 0.4765 - accuracy: 0.7802\n",
            "Test Loss (Filter Size 3): 0.47646859288215637\n",
            "Test Accuracy (Filter Size 3): 0.7801968455314636\n",
            "Epoch 1/3\n",
            "9697/9697 [==============================] - 103s 10ms/step - loss: 0.4708 - accuracy: 0.7758 - val_loss: 0.4545 - val_accuracy: 0.7857\n",
            "Epoch 2/3\n",
            "9697/9697 [==============================] - 87s 9ms/step - loss: 0.4058 - accuracy: 0.8156 - val_loss: 0.4541 - val_accuracy: 0.7883\n",
            "Epoch 3/3\n",
            "9697/9697 [==============================] - 88s 9ms/step - loss: 0.3394 - accuracy: 0.8506 - val_loss: 0.4876 - val_accuracy: 0.7786\n",
            "10000/10000 [==============================] - 29s 3ms/step - loss: 0.4876 - accuracy: 0.7786\n",
            "Test Loss (Filter Size 5): 0.48760154843330383\n",
            "Test Accuracy (Filter Size 5): 0.7786437273025513\n",
            "Epoch 1/3\n",
            "9697/9697 [==============================] - 112s 11ms/step - loss: 0.4708 - accuracy: 0.7758 - val_loss: 0.4470 - val_accuracy: 0.7895\n",
            "Epoch 2/3\n",
            "2509/9697 [======>.......................] - ETA: 1:10 - loss: 0.3992 - accuracy: 0.8197"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Define the model\n",
        "filter_sizes = [3, 5, 7]  # Define the filter sizes you want to try\n",
        "\n",
        "for filter_size in filter_sizes:\n",
        "    model = Sequential([\n",
        "        Embedding(max_words,100, input_length=max_len),\n",
        "        Conv1D(128, filter_size, activation='relu'),  # Vary the filter size here\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train_pad, y_train_onehot, epochs=3, batch_size=132, validation_data=(x_test_pad, y_test_onehot))\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test_pad, y_test_onehot)\n",
        "    print(f\"Test Loss (Filter Size {filter_size}):\", loss)\n",
        "    print(f\"Test Accuracy (Filter Size {filter_size}):\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take input text from the user\n",
        "input_text = input(\"Enter a text to predict its label: \")\n",
        "\n",
        "# Preprocess the input text\n",
        "input_text = preprocess_text(input_text)\n",
        "input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "input_sequence_pad = pad_sequences(input_sequence, maxlen=max_len)\n",
        "\n",
        "# Predict the label using the trained model\n",
        "predicted_probabilities = model.predict(input_sequence_pad)\n",
        "predicted_label = np.argmax(predicted_probabilities)\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJH8-AmfUzY6",
        "outputId": "5b6a8cf9-6cbd-4bec-8c73-45a95f6ab128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text to predict its label: I loved the movie! It was so engaging and entertaining\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Predicted Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of sentences to test\n",
        "num_sentences = 10\n",
        "# Loop through each sentence index\n",
        "for i in range(num_sentences):\n",
        "    # Take input text from your data\n",
        "    input_text = x_test[i]  # Assuming x_test contains the input text samples\n",
        "\n",
        "    # Get the actual target label from your data\n",
        "    actual_label = y_test[i]  # Assuming y_test contains the actual target labels\n",
        "\n",
        "    # Preprocess the input text\n",
        "    input_text = preprocess_text(input_text)\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence_pad = pad_sequences(input_sequence, maxlen=max_len)\n",
        "\n",
        "    # Predict the label using the trained model\n",
        "    predicted_probabilities = model.predict(input_sequence_pad)\n",
        "    predicted_label = np.argmax(predicted_probabilities)\n",
        "\n",
        "    # Print the predicted and actual labels for each sentence\n",
        "    print(f\"Sentence {i+1}:\")\n",
        "    print(\"Input Text:\", x_test[i])\n",
        "    print(\"Predicted Label:\", predicted_label)\n",
        "    print(\"Actual Label:\", actual_label)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYd-OQ2noz3l",
        "outputId": "da4add97-cb4a-4cde-b483-6ba8c497c36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "Sentence 1:\n",
            "Input Text: ['nkluvr4eva', 'poor', 'little', 'dumpling', 'holmdel', 'vids', 'really', 'trying', 'hope', 'dont', 'try', 'hard', 'tonight']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "Sentence 2:\n",
            "Input Text: ['bed', 'got', 'wake', 'hella', 'early', 'tomorrow', 'morning']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Sentence 3:\n",
            "Input Text: ['havent', 'able', 'listen', 'yet', 'speaker', 'busted']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Sentence 4:\n",
            "Input Text: ['remembers', 'solving', 'relatively', 'big', 'equation', 'two', 'unknown', 'total', 'pain', 'butt']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Sentence 5:\n",
            "Input Text: ['ate', 'much', 'feel', 'sick']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Sentence 6:\n",
            "Input Text: ['tried', 'purchase', 'parked', 'domain', 'godaddy', 'drain', 'kind', 'like', 'gambling']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Sentence 7:\n",
            "Input Text: ['lunch', 'come', 'eat']\n",
            "Predicted Label: 1\n",
            "Actual Label: 4\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Sentence 8:\n",
            "Input Text: ['got', 'back', 'tech', 'equine', 'medical', 'center', 'poor', 'lilly', 'stall', 'rest', 'least', 'another', 'month']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Sentence 9:\n",
            "Input Text: ['log', 'twitter', 'account', 'super', 'bummed']\n",
            "Predicted Label: 0\n",
            "Actual Label: 0\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Sentence 10:\n",
            "Input Text: ['tamaraschilling', 'adventure', 'need', 'life', 'glad', 'great', 'week', 'thanks', 'response']\n",
            "Predicted Label: 1\n",
            "Actual Label: 4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYL1bLzxr__m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}